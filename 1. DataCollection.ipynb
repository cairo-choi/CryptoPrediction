{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ab31ed-b886-4dff-be79-5bad2213c86d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#check python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8706cbf0-7240-4602-a183-1d765dc7c39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.Data Collection \n",
    "\n",
    "The data sources used in this project are divided into three main categories:\n",
    "- Asset Data: Historical OHLCV (Open, High, Low, Close, Volume) data for multiple financial assets were obtained through the Yahoo Finance API.\n",
    "- Hashrate Data: Blockchain computational metric,hashrate was collected via the Blockchain API.\n",
    "- Reddit Data: Submission data from a curated list of the top Bitcoin or cryptocurrency related subreddits (e.g., r/Bitcoin, r/Cryptocurrency, r/BitcoinMarkets) was collected using the PRAW Reddit API, supplemented by large-scale datasets from Academic Torrents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1982b9ec-f41b-4f73-9642-ae9b96dd1b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collection libraries\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede7950d-9e08-4919-b1fe-7b71b38f8c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 BaseCollection --- Parent Class \n",
    "\n",
    "BaseCollection serves as the parent class for all types of data collection. It defines the common attributes and core functionality that every data collection process must have.\n",
    "\n",
    "Instance attributes:\n",
    "  - name (str): Name of the data collection instance.\n",
    "  - save_path (str): Path where the collected data will be saved. e.g. It might be a path in your local file system or cloud-based storage location. In my case, DataBricks environment is employed, and the collected data is stored in underlying GCS.\n",
    "\n",
    "Instance functions:\n",
    "  - save_data(data): Saves the collected data to the specified path in Parquet format. Parquet is chosen because it efficiently compresses large datasets, significantly reducing storage costs. This is particularly important for large-scale data sources such as Reddit textual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2243445-75f5-4219-b29d-a6d3264790df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parent Class\n",
    "class BaseCollection():\n",
    "  def __init__(self, name, save_path):\n",
    "    self.name = name\n",
    "    self.save_path = save_path\n",
    "    self.spark = SparkSession.builder.getOrCreate()\n",
    "  \n",
    "  def save_data(self, data):\n",
    "    # if data is not spark datafram, then convert it to spark df\n",
    "    if not hasattr(data, \"write\"):\n",
    "      df = self.spark.createDataFrame(data)\n",
    "    else:\n",
    "      df = data  \n",
    "    save_path = f\"{self.save_path}/{self.name}\"\n",
    "    df.write.mode(\"overwrite\").parquet(save_path)\n",
    "\n",
    "    print(f'{self.name} has been sucessfully saved')\n",
    "\n",
    "  def normalize_to_utc(self, data):\n",
    "    if data.index.tz == dt.timezone.utc:\n",
    "      print(f'Datetime of {self.name} is utc')\n",
    "    else:\n",
    "      print(f'Datetime of {self.name} is not utc, it is converting...')\n",
    "      data.index = data.index.tz_localize(\"UTC\")\n",
    "\n",
    "      return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7588e8-8e64-4b16-bcc8-4433a06712dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 AssetCollection \n",
    "AssetCollection is a subclass of BaseCollection designed to collect financial asset data. Specifically, OHLCV (Open, High, Low, Close, Volume) information using the yfinance library. \n",
    "\n",
    "\n",
    "- Inherits instance attributes (**self.name, self.save_path**) and the **save_data** method\n",
    "- Adds functionality specific to financial data collection. \n",
    "- Adds functionality to check the number of rows in the fetched data.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b458aa87-59d8-4b63-ba2e-7f67b31fdc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collecting asset OHLCV data via yfinance API\n",
    "class AssetCollection(BaseCollection):\n",
    "  def __init__(self, name, save_path):\n",
    "    super().__init__(name, save_path) \n",
    "    self.data = None \n",
    "\n",
    "  def fetch_data(self, ticker, start_date = '2017-01-01', end_date = '2025-06-16', interval = '1d'):\n",
    "      try:\n",
    "          self.data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "          # check if datetime is utc or not\n",
    "          self.data = self.normalize_to_utc(self.data)  \n",
    "\n",
    "          # convert index to column, otherwise it will be lost when load data using spark\n",
    "          self.data = self.data.reset_index()\n",
    "          \n",
    "          return self.data\n",
    "       \n",
    "      except Exception as e:\n",
    "          print(f\"Error fetching data for {ticker}: {e}\")\n",
    "          self.data = None\n",
    "          return None    \n",
    "      \n",
    "  def __len__(self):\n",
    "    if self.data is None:\n",
    "        return 0\n",
    "    elif hasattr(self.data, \"count\"):   # if spark df\n",
    "        return self.data.count()\n",
    "    else:                        # pandas df, or list\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cf71ff-9ea3-4dc2-96d6-b88e6703fedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.3 HashRateCollection\n",
    "\n",
    "Explanation (from https://www.blockchain.com/explorer/charts/hash-rate)\n",
    "- Mining hashrate is a key security metric. The more hashing (computing) power in the network, the greater its security and its overall resistance to attack. Although Bitcoinâ€™s exact hashing power is unknown, it is possible to estimate it from the number of blocks being mined and the current block difficulty.\n",
    "\n",
    "Notes (from https://www.blockchain.com/explorer/charts/hash-rate)\n",
    "- Daily numbers (raw values) may periodically rise or drop as a result of the randomness of block discovery : even with a hashing power constant, the number of blocks mined can vary in day. Our analysts have found that looking at a 7 day average is a better representation of the underlying power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c41490-f640-437e-9207-bcad371657c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collecting hash rate data via Blockchain API\n",
    "class HashrateCollection(BaseCollection):\n",
    "  def __init__(self, name, save_path):\n",
    "    super().__init__(name, save_path) \n",
    "    self.data = None\n",
    "    self.df = None \n",
    "\n",
    "  def fetch_data(self, sampled = False, start_date = '2017-01-01', end_date = '2025-06-15'):\n",
    "      url = \"https://api.blockchain.info/charts/hash-rate\"\n",
    "      params = {\n",
    "            'timespan': 'all',  # Since the Blockchain.com API does not support start and end parameters, the fetched data have to be filtered on the client side to select the desired date range.\n",
    "            'format': 'json',\n",
    "            'sampled': str(sampled).lower()\n",
    "            }\n",
    "      try:\n",
    "          response = requests.get(url, params=params)\n",
    "          response.raise_for_status() # if response is not 200, raise exception\n",
    "          self.data = response.json() # collected as json fomat\n",
    "          self.df = pd.DataFrame(data=self.data['values'])  # create dataframe from 'values' in json data \n",
    "          self.df = self.df.rename(columns={'x': 'Date', 'y': 'HashRate'})   # x -> Date, y -> HashRate\n",
    "          self.df['Date'] = pd.to_datetime(self.df['Date'], unit='s') # convert Date to datetime\n",
    "          self.df.set_index('Date', inplace=True) # set Date as index\n",
    "\n",
    "          # check if datetime is utc or not\n",
    "          self.df = self.normalize_to_utc(self.df)\n",
    "\n",
    "          # filter by start_date and end_date\n",
    "          self.df = self.df[(self.df.index >= start_date) & (self.df.index <= end_date)]\n",
    "\n",
    "          # convert index to column, otherwise it will be lost when load data using spark\n",
    "          self.df = self.df.reset_index()\n",
    "\n",
    "          return self.df\n",
    "      \n",
    "      except Exception as e:\n",
    "          print(f\"Error fetching data: {e}\")\n",
    "          self.data = None\n",
    "          return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "582b5367-08c8-4472-9f77-3340c86b8a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.4 RedditCollection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290ac7fa-bcb5-42dd-bdd6-eefb45a83cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a6de24-5dc4-48fd-a487-431e2029dee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.5 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f04e7cc-b2b5-419c-bedd-4095711a58a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define save path\n",
    "save_path = '/mnt/demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43561131-a18f-4fa9-9e67-9bd85b00314e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set btc, eth, sp500, xrp, ltc tickers \n",
    "btc_ticker = 'BTC-USD'\n",
    "eth_ticker = 'ETH-USD'\n",
    "sp500_ticker = '^GSPC'\n",
    "xrp_ticker = 'XRP-USD'\n",
    "ltc_ticker = 'LTC-USD'\n",
    "\n",
    "tickers = [btc_ticker, eth_ticker, sp500_ticker, xrp_ticker, ltc_ticker]\n",
    "\n",
    "for ticker in tickers:\n",
    "  asset = AssetCollection(ticker, save_path)\n",
    "  data = asset.fetch_data(ticker)\n",
    "  print(data)\n",
    "  # asset.save_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff6bf19-be79-4597-86c4-1f538c393069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect hashrate\n",
    "hashrate = HashrateCollection('hashrate', save_path)\n",
    "data = hashrate.fetch_data()\n",
    "print(data)\n",
    "# Save data on the defined path\n",
    "# hashrate.save_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4769522-6ac1-4369-8e8a-9b95998de9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. DataCollection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
